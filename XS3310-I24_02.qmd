---
title: "Introducción-2"
subtitle: "XS3310 Teoría Estadística <br> I Semestre 2024"  
author: "Prof. Shu Wei Chou Chen"
institute: "Escuela de Estadística, UCR."
format:
  revealjs: 
    theme: sky
    slide-number: true
    fig-width: 3
    fig-height: 3
fontsize: 20pt
---


## Modelos estadísticos y familias de modelos

**Ejemplo:**

Una empresa produce componentes eléctricos y el interés es medir la vida útil del componente (en años). Suponemos que la vida útil de los componentes sigue una distribución exponencial con parámetro $\beta>0$.

$$f(y)= \begin{cases}\frac{1}{\beta} e^{-y/\beta}, & 0 < y < \infty, \\ 0, & y \leq 0, \end{cases}$$ - Si $\beta$ fuera conocida, conocemos el comportamiento de la vida útil del componente, por ende, podemos calcular diferentes medidas y probabilidades.

-   ¿Si no conocemos $\beta$?

## Modelos estadísticos y familias de modelos

-   ¿Si no conocemos $\beta$? (pero sabemos que $\beta>0$)
-   La idea es estimarlo bajo algunos "supuestos".
-   Si $Y_1,Y_2,...,Y_n$ es una secuencia de observaciones de vidas útiles de $n$ componentes.
-   **Supuestos:**
    1.  Suponga que $Y_1,Y_2,...,Y_n$ consiste en $n$ variables aleatorias i.i.d. con distribución exponencial con parámetro $\beta$.
    2.  Sabemos que $E(Y)=\beta$.
    3.  Con las observaciones disponibles $Y_1,Y_2,...,Y_n$, podríamos calcular una media muestral: $\bar{Y}=\sum\limits_{i=1}^n \frac{Y_i}{n}$.
-   **Ejercicio:**

1.  Analice si $\bar{Y}$ puede aproximarse a $\beta$.
2.  Identifique los supuestos.

## Modelos estadísticos y familias de modelos

**Definición:** Un *modelo estadístico* consiste en una identificación de variables aleatorias de interés, la especificación de una distribución conjunta, o una familia de posibles distribuciones conjuntas para unas variables aleatorias observables, la identificación de uno o varios parámetros de dichas distribuciones son desconocidas.


## Modelos estadísticos y familias de modelos

**Ejemplos:** Para el caso de componentes eléctricos, se tienen las variables aleatorias $Y_1,...,Y_n$ cuya **distribución conjunta** es:

$$f(y_1,...,y_n|\beta)= \prod_{i=1}^n f(y_i|\beta),$$

en donde $f(y_i)$ es la densidad de la distribución exponencial con parámetro $\beta>0$, *i.e.*

$$f(y)= \begin{cases}\frac{1}{\beta} e^{-y/\beta}, & 0 < y < \infty, \\ 0, & y \leq 0. \end{cases}$$

La familia de posibles distribuciones conjuntas es $$
\left\lbrace  f(y_1,...,y_n|\beta), \beta >0 \right\rbrace.
$$

## Modelos estadísticos y familias de modelos

**Ejercicio (caso discreto):**

En una urna con $120$ bolas se encuentran bolas de color rojo y azul únicamente. El interés es conocer la proporción (desconocida) de bolas rojas, denotada por $p$.

-   Sabemos que $p \in [0,1]$.

-   Sea $Y_1,Y_2,...,Y_n$, una secuencia de observaciones con reemplazo que denote 1 como roja y 0 como azul.

-   Identifique el modelo estadístico.

## Estadística paramétrica y no paramétrica

**Definición:** La *inferencia estadística* es el procedimiento de producir afirmaciones probabilísticas sobre alguna (o toda) parte del modelo estadístico.

**Definición:** La *estadística paramétrica* consiste en realizar inferencia cuando el modelo estadístico puede ser representado por medio de uno o varios (finitos) parámetros desconocidos de una distribución.

**Definición:** dos definiciones informales de la *estadística no paramétrica*:

1.  Inferencia en modelos estadísticos que son de dimensión infinita.
2.  Conjunto de herramientas cuyo objetivo es realizar inferencia usando los menos supuestos posibles.

## Estadística paramétrica y no paramétrica

**Ejemplos:**

::: incremental
-El ejemplo de componentes eléctricos corresponde a estadística paramétrica.

-   Sea $X_1,...,X_n$ una muestra aleatoria de una población con función de distribución $F$. Realizar inferencia sobre la función de distribución $F(x)=P(X\leq x)$ y la función de densidad $f(x)=F'(x)$.
    -   Como $F$ y $f$ no puede ser representada por medio de un conjunto finito de parámetros, la inferencia es no paramétrica.
-   Sea $(Y_1, X_1), ... ,(Y_n,X_n)$ una muestra aleatoria de dos variables aleatorias. Realizar inferencia sobre un modelo de regresión lineal $Y_i=\beta X_i+\epsilon_i$, $\epsilon_i \sim N(0,\sigma^2)$.
    -   La inferencia es paramétrica, pues el modelo estadístico puede ser representado por $\theta=(\beta,\sigma^2)$.
-   Sea $(Y_1, X_1), ... ,(Y_n,X_n)$ una muestra aleatoria de dos variables aleatorias. Realizar inferencia sobre un modelo de regresión $Y_i=f(X_i)+\epsilon_i$.
    -   La inferencia es no paramétrica.
:::

## Propiedades de la media y variancia muestral

Sea $X_{1}, \ldots, X_{n}$ una m.a. cuya distribución poblacional tiene media $\mu$ y varianza $\sigma^{2}$. Definan la *media muestral* y la *Varianza muestral*:

$$\bar{X}_{n}=\bar {X}=\sum_{i=1}^n \frac{X_i}{n}, ~~~\text{y}~~~  S^{2} = \frac{\sum_{i=1}^n{\left(X_{j}-\bar{X}\right)^2}}{n-1}.$$

::: incremental
Entonces,

-   $E\left(\overline{X}_{n}\right)=\mu$ y $\operatorname{Var}\left(\bar{X}_{n}\right)=\frac{\sigma^{2}}{n}$

-   ¿Qué se sabe sobre la variancia muestral?

    -   $E\left(S^2\right)=\sigma^2$.
:::

## Propiedades de la media y variancia muestral

$$\begin{eqnarray*}	
E\left(S^2\right)&=& E\left[\frac{\sum{\left(X_{j}-\overline{X}\right)^2}}{n-1}\right] = \frac{1}{n-1}E\left[\sum{\left[\left(X_{j}-\mu\right)-\left(\overline{X}-\mu\right)\right]^2}\right] \\ &=& \frac{1}{n-1}E\left[\sum{\left[\left(X_{j}-\mu\right)^{2}-2\left(X_{j}-\mu\right)\left(\overline{X}-\mu\right) + \left(\overline{X}-\mu\right)^{2} \right]}\right] \\ &=&\frac{1}{n-1}E\left[\sum{\left(X_{j}-\mu\right)^{2}}-2\left(\overline{X}-\mu\right)\sum{\left(X_{j}-\mu\right)} + \sum{\left(\overline{X}-\mu\right)^{2}}\right] \\ &=&\frac{1}{n-1}E\left[\sum{\left(X_{j}-\mu\right)^{2}}-2n\left(\overline{X}-\mu\right)^2 + n\left(\overline{X}-\mu\right)^{2}\right] \\ &=& \frac{1}{n-1}\left[\sum{E\left[\left(X_{j}-\mu\right)^{2}\right]}-nE\left[\left(\overline{X}-\mu\right)^2\right]\right] \\ &=& \frac{1}{n-1}\left(n\sigma^{2} - n\frac{\sigma^2}{n}\right)=\frac{1}{n-1}\left(n\sigma^{2} - \sigma^{2}\right)=\frac{\sigma^{2}\left(n-1\right)}{n-1}=\sigma^2
\end{eqnarray*}$$




## Modos de convergencia

### Desigualdad de Markov

Si $X$ es una v.a. tal que $\operatorname{Pr}(X \geq 0)=1$. Entonces para cada número real $t>0$, se tiene que

```{=tex}
\begin{equation}
\operatorname{Pr}(X \geq t) \leq \frac{E(X)}{t}.
\end{equation}
```
### Desigualdad de Chebyshev

Si $X$ es una v.a. que cumple que $\operatorname{Var}(X)$ existe. Entonces para cada número $t>0$

$$
\operatorname{Pr}(|X-E(X)| \geq t) \leq \frac{\operatorname{Var}(X)}{t^{2}}.
$$

## Modos de convergencia

### Convergencia en probabilidad

Decimos que una secuencia de v.a. $Z_{1}, Z_{2}, \ldots$ converge a $b$ *en probabilidad* si para cada número $\varepsilon>0$,

$$\lim _{n \rightarrow \infty} \operatorname{Pr}\left(\left|Z_{n}-b\right|<\varepsilon\right)=1$$

Esta propiedad se denota como

$$
Z_{n} \stackrel{p}{\longrightarrow} b.
$$

## Propiedades de la media muestral

### Ley de los grandes números

Suponga que $X_{1}, \ldots, X_{n}$ es una muestra de una distribución con media $\mu$ y varianza finita. Sea $\bar{X}_{n}$ la media muestral. Entonces, \begin{equation*}
\bar{X}_{n} \stackrel{p}{\longrightarrow} \mu
\end{equation*}

¿Cómo se puede probar este resultado usando la desigualdad de Chebyshev?

## Propiedades de la media muestral

### Convergencia en distribución

Suponga que se tienen una secuencia de funciones de distribución $F_1, \ldots, F_n, \ldots$ para la secuencia de v.a. $X_1,\ldots,X_n,\ldots$. Se dice que la secuencia de v.a. $X_1,\ldots,X_n,\ldots$ converge *en distribución* a $X^*$, cuya función de distribución es $F^*$, si

\begin{equation*}
\lim _{n \rightarrow \infty} F_{n}(x)=F^{*}(x).
\end{equation*} Se denota también como \begin{equation*}
X_n \stackrel{d}{\longrightarrow} X^*.
\end{equation*}

## Propiedades de la media muestral

### Teorema del límite central

Si $X_1,\ldots, X_n$ es una muestra aleatoria de tamaño $n$, y la distribución tiene media $\mu$ y varianza $0<\sigma^2< \infty$. Entonces se cumple que

$$Z_{n} = \frac{\overline{X}_n-\mu}{\frac{\sigma}{\sqrt{n}}} \stackrel{d}{\longrightarrow} N\left(0,1\right) \quad si\quad n \rightarrow \infty$$ o lo que es equivalente $$\overline{X}_n \xrightarrow{\text{d}} N\left(\mu, \frac{\sigma^2}{n}\right) \quad si\quad n \rightarrow \infty.$$

## Propiedades de la media muestral

### Relaciones entre los modos de convergencia

Se puede probar que \begin{equation*}
\text{Convergencia en Probabilidad} \implies \text{Convergencia en distribución}
\end{equation*}

Para un resumen más detallado, les dejo este video donde se explica mucho mejor

<https://www.youtube.com/watch?v=bTMnnrw0v2Y>

