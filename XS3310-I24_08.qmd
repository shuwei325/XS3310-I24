---
title: "Estimación Puntual-5"
subtitle: "XS3310 Teoría Estadística <br> I Semestre 2024"  
author: "Prof. Shu Wei Chou Chen"
institute: "Escuela de Estadística, UCR."
format:
  revealjs: 
    theme: sky
    slide-number: true
    fig-width: 3
    fig-height: 3
fontsize: 20pt
---

# ¿Qué vamos a discutir hoy?

<!-- - Algoritmo Esperanza-Maximización.  -->
- Distribución de EMV en muestras grandes.
-	Método delta y aplicaciones.

<!-- ## Algoritmo Esperanza-Maximización.  -->



## Teorema del límite central

Si $X_1,\ldots, X_n$ es una muestra aleatoria de tamaño $n$, y la distribución tiene media $\mu$ y varianza $0<\sigma^2< \infty$. Entonces se cumple que

$$Z_{n} = \frac{\overline{X}_n-\mu}{\frac{\sigma}{\sqrt{n}}} \stackrel{d}{\longrightarrow} N\left(0,1\right) \quad si\quad n \rightarrow \infty$$ o lo que es equivalente $$\overline{X}_n \xrightarrow{\text{d}} N\left(\mu, \frac{\sigma^2}{n}\right) \quad si\quad n \rightarrow \infty.$$



## Información de Fisher.


**Definición 2.11:** La información de Fisher de una muestra aleatoria $X_1,...,X_n$ se define como
$$I(\theta) = \left[nE\left(\frac{\partial\ln f_{X}(x)}{\partial \theta}\right)^2\right].$$ 
Bajo el supuesto de que la segunda derivada de $\ln f_{X}(x)$ existe y que se puede intercambiar el orden de las derivadas con las integrales^[Condición de regularidades.], entonces $I(\theta)$ se puede expresar como:

$$I(\theta) = \left[nE\left(-\frac{\partial^{2}\ln(f_{X}(x))}{\partial \theta^{2}}\right)\right].$$ 

## Información de Fisher.

**Ejemplo:** Encuentre la información de Fisher de una muestra aleatoria cuya población es Bernoulli con parámetro $p$.

**Solución:** 

$$\ln f_{X}(x)= x \ln p + (1-x) \ln (1-p)$$
$$\frac{\partial\ln f_{X}(x)}{\partial \theta}=\frac{x}{p}-\frac{1-x}{1-p}=\frac{x-p}{p(1-p)}$$
$$E\left[ \left( \frac{\partial\ln f_{X}(x)}{\partial \theta}\right)^2 \right]=E\left[\frac{x^2-2xp+p^2}{p^2(1-p)^2}\right]=\frac{1}{p^2(1-p)^2}\left(p-2p^2+p^2\right)=\frac{1}{p(1-p)},$$
pues $E(X^2)=Var(X)+[E(X)]^2=pq+p^2=p$.

$$\Rightarrow I(\theta) = \left[nE\left(\frac{\partial\ln f_{X}(x)}{\partial \theta}\right)^2\right]=\frac{n}{p(1-p)}.$$

---

Por otro lado,

$$\frac{\partial^{2}\ln(f_{X}(x))}{\partial \theta^{2}}=-\left[\frac{x}{p^2}+\frac{1-x}{(1-p)^2} \right]$$


$$\Rightarrow I(\theta) = nE\left(-\frac{\partial^2\ln f_{X}(x)}{\partial \theta^2}\right)=n E \left[\frac{x}{p^2}+\frac{1-x}{(1-p)^2} \right]=n\left[\frac{1}{p}+\frac{1}{1-p}\right]=\frac{n}{p(1-p)}.$$
Ambos coinciden y es el inverso de la variancia de $\bar{X}$.


## Distribución de los EMV en muestras grandes.

**Teorema 2.11:** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria sobre una población con función de densidad $f(x|\theta)$, de tal forma que $f(x|\theta)$ satiface ciertas condiciones de "regularidad". Para cada muestra de tamaño $n$, sea $\hat{\theta}_n$ el EMV de $\theta$. Tenemos que para $n \rightarrow \infty$, la distribución de $\hat{\theta}_n$ converge a una distirbución normal con media $\theta$ y variancia $1/[I(\theta)]$. En otras palabras,

$$\left[I(\theta)\right]^{1/2}(\hat{\theta}_n - \theta) \stackrel{d}{\longrightarrow} N(0,1)$$

**Nota:** Recuerde que $I(\theta)$ es la Información de Fisher.

$$I(\theta) = \left[nE\left(-\frac{\partial^{2}\ln(f_{X}(x|\theta))}{\partial \theta^{2}}\right)\right].$$

## Distribución de los EMV en muestras grandes.

**Ejemplo:** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria sobre una población con distribución normal con media 0 y desviación estándar $\sigma$ desconocida. Encuentre la distribución asintótica del EMV de $\sigma$.

**Solución:** Demostrar que el EMV de $\sigma$ es: 

$$\hat{\sigma} = \left[\frac{\sum\limits_{i=1}^n X_{j}^2}{n}\right]^{1/2}.$$
Luego, la función de densidad es:
$$
f(x|\sigma)= \frac{1}{\sqrt{2\pi}\sigma}  e^{-\frac{ x^2}{2\sigma^2}}.
$$


---

y su log función de densidad es:
$$\ln f(x|\sigma) = -\frac{1}{2} \ln(2\pi) - \ln(\sigma) -\frac{x^2}{2\sigma^2}$$

$$\Rightarrow \frac{\partial \ln f(x|\sigma)}{\partial \sigma} = - \frac{1}{\sigma} +\frac{x^2}{\sigma^3}$$

$$\Rightarrow \frac{\partial^2 \ln f(x|\sigma)}{\partial \sigma^2} = \frac{1}{\sigma^2} -\frac{3x^2}{\sigma^4}$$


$$I(\sigma) = \left[nE\left(-\frac{\partial^{2}\ln(f_{X}(x|\sigma))}{\partial \sigma^{2}}\right)\right]=nE\left(-\frac{1}{\sigma^2} +\frac{3X^2}{\sigma^4}\right)=n\left[\frac{1}{\sigma^2} -\frac{3E(X^2)}{\sigma^4}\right]=\frac{2n}{\sigma^2}.$$

Se concluye que la distribución asintótica del EMV de $\sigma$ es normal con media $\sigma$ y variancia $\frac{\sigma^2}{2n}$, o bien


$$\left[\frac{2n}{\sigma^2} \right]^{1/2}(\hat{\sigma}_n - \sigma) \stackrel{d}{\longrightarrow} N(0,1)$$

## Método Delta 

**Teorema 2.12:** Si $X_1,X_2,\dots$ es una secuencia de variables aleatorias y con alguna distribución continua. Sea $\theta\in \mathbb R$ y $\{a_n\}$ sucesión de números positivos tal que $a_n \rightarrow \infty$ y $\hat{\theta}$ un estimador para estimar $\theta$.  
Suponga que $a_n(\hat{\theta}-\theta) \xrightarrow{d} N(0,\sigma^{2})$. Si $g$ es una función tal que $g^{\prime}(\theta)\ne 0$, entonces

$$a_n\left[g(\hat{\theta})-g(\theta)\right] \xrightarrow{d} N\left(0,\sigma^2 \left[g^{\prime}\left(\theta\right)\right]^{2}\right)$$



## Método Delta 

**Ejemplo**. $X_1,X_2,\dots$ una muestra aleatoria cuya población tiene media $\mu \neq 0$ y varianza $\sigma^2$. ¿Cuál es la distribución asintótica de $\bar{X}^{-1}$?

Sea $g(\mu) = \dfrac 1\mu$ con $\mu\neq0$. $g$ una función tal que $g'(\mu)\neq 0$. 
Por el TLC, 
$$ \sqrt{n}(\overline{X}_{n} -\mu)\xrightarrow{d}N(0,\sigma^{2})$$



Entonces por el método Delta $\sqrt{n}\left[g(\bar X_n)-g(\mu)\right]\xrightarrow{d}N\left(0,\sigma^2 \left(g^{\prime}(\mu)\right)^{2}\right)$

Para la función $g$ seleccionada se tiene que  $g'(\mu) = -\dfrac{1}{\mu^2}$. Entonces por el método Delta

$$ \sqrt{n}\bigg[\dfrac{1}{\overline{X}_n}-\dfrac 1\mu\bigg]\xrightarrow{d}N\left(0,\dfrac{\sigma ^2}{\mu ^4}\right) $$

## Método Delta 

**Ejemplo:** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria sobre una población con distribución normal con media 0 y variancia $\sigma^2$ desconocida. Encuentre la distribución asintótica del EMV de $\sigma$.

**Solución:** Ya se demostró que el EMV de $\sigma^2$ es

$$\hat{\sigma}^{2} = \frac{\sum_{i=1}^n X_{j}^2}{n}$$
Por el principio de invariancia, tenemos que $\sqrt{\hat{\sigma}^2}$ es el EMV de $\sigma$.

Tomando $\omega=\sigma^2$, el logarítmo de la función de densidad es 

$$\ln f(x,\omega) = -\frac{1}{2} \ln(2\pi) -\frac{1}{2} \ln(\omega) -\frac{x^2}{2\omega}$$

--- 

Recuerde que tomando $\omega=\sigma^2$
$$\frac{\partial^{2} \ell(\omega)}{\partial \omega^{2}} = \frac{1}{2\omega^2} - \frac{x^2}{\omega^3},$$
Entonces, 

$$I(\omega) = \left[nE\left(-\frac{\partial^{2}\ln(f_{X}(x|\omega))}{\partial \omega}\right)\right]= nE\left(-\frac{1}{2\omega^2} +\frac{X^2}{\omega^3}\right)$$
$$=n\left[-\frac{1}{\omega^2} +\frac{E(X^2)}{\omega^3}\right]=\frac{n}{2\omega^2}=\frac{n}{2\sigma^4}.$$
Finalmente, tenemos que 
$$\left[\frac{n}{2\sigma^4} \right]^{1/2}(\hat{\sigma}^2_n - \sigma^2) \stackrel{d}{\longrightarrow} N(0,1)$$
En otras palabras,

$$n^{1/2}(\hat{\sigma}^2_n - \sigma^2) \stackrel{d}{\longrightarrow} N(0,2\sigma^4)$$


---

$$n^{1/2}(\hat{\sigma}^2_n - \sigma^2) \stackrel{d}{\longrightarrow} N(0,2\sigma^4)$$
Aplicando el Método Delta, sea $g(\omega)=\sqrt{\omega}$. Su derivada es

$$g'(\omega)=\frac{1}{2\sqrt{\omega}}$$
Entonces,

$$n^{1/2}(\hat{\sigma}_n - \sigma) \stackrel{d}{\longrightarrow} N\left(0,2\sigma^4 \left[ \frac{1}{2\sqrt{\sigma^2}} \right]^2 \right)=N\left(0,\frac{\sigma^2}{2} \right)$$

Es decir, la distribución asintótica de $\hat{\sigma}_n$ es una normal con media $\sigma$ y con variancia $\frac{\sigma^2}{2n}$.

**Nota:** Este resultado coincide con el resultado de la diapositiva 9.


## Estabilización de la varianza 

Es posible definir la función $g$ de modo que la varianza no dependa del parámetro desconocido. 

\begin{equation*}
g(\mu)=\int_{a}^{\mu} \frac{d x}{s(x)^{1 / 2}}
\end{equation*}

donde $s(x)$ es toda la expresión de la varianza y $a$ es cualquier valor que haga la integral finita y fácil de calcular. 



## Estabilización de la varianza 

**Ejemplo:**

Suponga que $X_1, X_2, ..., X_n$ es una muestra aleatoria Poisson con parámetro $\theta$. Sea $\overline{X}$ la media muestral. 

Sabemos que $\mu = \sigma ^2 =\theta$. 

$$n^{1 / 2}\left(\bar{X}_{n}-\theta\right) \stackrel{d}{\longrightarrow} N(0,\theta)$$

**OJO: el paramétro $\theta$ es desconocido!**

La varianza asintótica es $\theta$. 



## Estabilización de la varianza 

Entonces la

\begin{equation*}
g(\theta)=\int_{0}^{\theta} \frac{d x}{x^{1 / 2}}=2 \theta^{1 / 2}
\end{equation*}

Aplicando el método Delta, se tiene que 

$$g'(\theta) = 2\frac{1}{2} \frac{1}{\theta ^{1 / 2}} = \frac{1}{\theta ^{1 / 2}} $$

Por lo que obtenemos 

\begin{align*}
n^{1 / 2}\left(g(\bar{X}_{n})-g(\theta)\right)  
&\stackrel{d}{\longrightarrow} N\left(0,\theta \left(\frac{1}{\theta ^{1 / 2}} \right)^{2} \right) \\  
n^{1 / 2}\left(2(\bar{X}_{n})^{1/2}-2(\theta)^{1/2}\right)  
&\stackrel{d}{\longrightarrow} N(0,1) 
\end{align*}


**Después de la transformación, la distribución asintótica (normal) no depende del paramétro.**



## Lab02


[Lab02](lab02.html)



# ¿Qué discutimos hoy?

Propiedades de muestras grandes del EMV y Método Delta.

Hasta aquí termina el tema 2.


## Ejercicio

1- Suponga que $X_1, X_2, ..., X_n$ es una muestra aleatoria exponencial con paramétro $\beta$. Se sabe que la esperanza es $\beta$ y varianza $\beta ^2$. Encuentre una función $g(\beta)$ de modo que la convergencia en distribución de $g(\overline{X})$ no depende de $\theta$. 

![](figs/ejercicio.png)


