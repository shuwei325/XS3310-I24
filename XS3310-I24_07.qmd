---
title: "Estimación Puntual-4"
subtitle: "XS3310 Teoría Estadística <br> I Semestre 2024"  
author: "Prof. Shu Wei Chou Chen"
institute: "Escuela de Estadística, UCR."
format:
  revealjs: 
    theme: sky
    slide-number: true
    fig-width: 3
    fig-height: 3
fontsize: 20pt
---

# ¿Qué vamos a discutir hoy?

- Estimadores de momentos
- Estimador de máxima verosimilitud (EMV)



## Estimadores de momentos
	
**Definición 2.8:** *Momento al origen*. Si $X$ es una variable aleatoria con distribución conocida, se define el $k$-ésimo momento al origen como: $\mu_{k}^{\prime} = E\left[X^{k}\right]$
	
**Definición 2.9:** *Momento central*. Si $X$ es una variable aleatoria con distribución conocida y media $\mu$, se define el $k$-ésimo momento central como:
$$\mu_{k} = E\left[(X-\mu)^{k}\right]$$
Para este tema también vamos a definir un nuevo tipo de momento, que llamaremos momento muestral:
	
**Definición 2.10:** *Momento muestral*. Si $X_{1}, X_{2}, ... , X_{n}$ es una muestra aleatoria de una distribución conocida, entonces se define el $k$-ésimo momento muestral como:
$$m_{k}^{\prime} = \frac{1}{n} \sum_{j=1}^{n} X_{j}^{k}$$


## Estimadores de momentos
	
Si nos basamos en la idea de que los momentos muestrales son buenos estimadores de los momentos poblaciones al origen, entonces para determinar el estimador de momentos (EM) en relación con un parámetro desconocido $\theta$, se resuelve la ecuación generada al igualar el momento (o momentos) poblacional al origen con el momento muestral y resolviendo para $\theta$. 
	


## Estimadores de momentos

**Ejemplo.** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria tal que $X_{j} \sim Unif(0, \theta)$. Encuentre el estimador de momentos para $\theta$. 



**Solución.** Empezando por el momento al origen tenemos:
$$\mu_{1}^{\prime} = E\left[X^{1}\right] = E[X] = \frac{\theta}{2}$$

El primer momento muestral siempre va a ser $\overline{X}$ por lo que en este caso tenemos la siguiente igualdad:
	
$$\mu_{1}^{\prime} = m_{1}^{\prime}$$
	
$$\Rightarrow \frac{\theta}{2} = \overline{X}$$
	
$$\Rightarrow \hat{\theta} = 2\overline{X}$$
	
Por lo tanto concluimos que $\hat{\theta} = 2\overline{X}$ es el estimador de momentos para $\theta$. Se puede demostrar que este es un estimador insesgado y consistente para $\theta$. 



## Estimadores de momentos
	
		
**Ejemplo.** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria tal que $X_{j} \sim Gamma(\alpha, \beta)$. Determine un estimador de momentos para $\alpha$ y para $\beta$. 
	
**Solución.** En el los casos en que se requiere estimar más de un parámetro se debe utilizar un número de igualdades similar al número de parámetros desconocidos. En este caso tenemos:
	
$$\begin{array}{ll} \mu_{1}^{\prime} = E(X) = \alpha\beta & m_{1}^{\prime} = \overline{X} \\ \mu_{2}^{\prime} = E(X^{2}) = \alpha \beta^{2} + (\alpha\beta)^{2} & m_{2}^{\prime} = \frac{1}{n} \sum_{j=1}^{n} X_{j}^{2} = \overline{X^2} \end{array}$$
	
Ahora debemos resolver el siguiente sistema de ecuaciones:
	
$$\alpha\beta = \overline{X}$$
$$\alpha \beta^{2} + (\alpha\beta)^{2}  = \overline{X^2}$$
	
## Estimadores de momentos


Podemos sustituir $\alpha\beta$ de la primera ecuación en la segunda y despejar $\beta$:
	
$$\overline{X}\beta + {\overline{X}}^{2} = \overline{X^2}$$

$$\Rightarrow \hat{\beta} = \frac{\overline{X^2} - {\overline{X}}^{2}}{\overline{X}}$$



## Estimadores de momentos
	
Sustituyendo este resultado en la primera ecuación podemos obtener que
	
$$\hat{\alpha} = \frac{{\overline{X}}^{2}}{\overline{X^2} - {\overline{X}}^{2}}$$
	
Concluimos que $\hat{\alpha}$ y $\hat{\beta}$ son los estimadores de momentos de $\alpha$ y $\beta$ respectivamente. 
	
Si se quisieran simplificar un poco estas expresiones se puede demostrar que 
	
$$\hat{\alpha} = \frac{n{\overline{X}}^{2}}{(n-1)S^{2}}$$
	
$$\hat{\beta} = \frac{(n-1)S^{2}}{n\overline{X}}$$



## Estimadores de máxima verosimilitud
	
Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria sobre una población con distribución que incluye parámetros $\theta_{1}, \theta_{2}, ... , \theta_{k}$. La función de verosimilitud $\mathcal{L}(x_{1}, ... , x_{n}|\theta_{1}, \theta_{2}, ... , \theta_{k})$, mejor expresada como $\mathcal{L}(\theta_{1}, \theta_{2}, ... , \theta_{k})$, se dice que es una función de los parámetros para un específico resultado de la muestra aleatoria. 
	
**Ejemplo.** Suponga que tenemos una función de densidad discreta (que toma valores 1, 2, 3 y 4) que depende de un parámetro $\theta$ que solo puede tomar tres valores (0,1,2). Su función de probabilidad viene dada en la siguiente tabla: 
	
Función de probabilidad

y             | 1    | 2    | 3    | 4     
--------------| -----|------|------|------
$f(y;\theta)$ |
$f(y;0)$      | .980 | .005 | .005 | .010  
$f(y;1)$      | .100 | .200 | .200 | .500  
$f(y;2)$      | .098 | .001 | .001 | .900  



## Estimadores de máxima verosimilitud
	
		
Supongamos que obtuvimos una muestra aleatoria de tamaño cuatro donde observamos los siguientes datos: 4,4,3,4. ¿Cuál es el valor de $\theta$ que máximiza la función de verosimilitud? 
	
**Solución**. Debemos obtener la verosmilitud para la muestra obtenida para cada uno de los valores de $\theta$. El valor de $\theta$ que genere la máxima verosimilitud será el estimador de máxima verosimilitud. 
	
Recordemos que la función de verosimilitud es la probabilidad conjunta de la muestra observada. En este caso viene dada por:

\begin{align*}
\mathcal{L}(\theta) = P(Y=4|\theta) \cdot P(Y=4|\theta) \cdot P(Y=3|\theta) \cdot P(Y=4|\theta) 
&= P(Y=4|\theta)^{3} \cdot P(Y=3|\theta)
\end{align*}
Debemos encontrar este valor para cada uno de los posibles valores de $\theta$:
	
$$\mathcal{L}(0) = (0.010)^{3} \cdot (0.005) = 5 \cdot 10^{-09}$$
$$\mathcal{L}(1) = (0.500)^{3} \cdot (0.200) = 0.025$$
$$\mathcal{L}(2) = (0.900)^{3} \cdot (0.001) = 0.00729$$



## Estimadores de máxima verosimilitud
	
En este caso obtenemos la mayor verosimilitud cuando $\theta=1$, por lo tanto el estimador de máxima verosimilitud sería $\hat{\theta} = 1$.
	
En el caso en que $\theta$ sea una variable cuyo dominio es continuo, para encontrar el estimador de $\theta$ basta con optimizar la verosimilitud siempre que existan los máximos correspondientes. 

**Ejemplo.** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria tal que $X_{j} \sim Bernoulli(p)$. Determinar el estimador de máxima verosimilitud (EMV) para $p$. 
	
**Solución.** Ya anteriormente habiamos obtenido la función de verosimilitud de una muestra aleatoria Bernoulli,
	
$\mathcal{L}(p) = p^{u}(1-p)^{n-u} \quad donde \quad u = \sum_{j=1}^{n} x_{j}$
	


## Estimadores de máxima verosimilitud
	
Derivamos esta función con respecto a $p$ e igualamos a cero. No obstante podemos notar como esto va a ser un proceso tedioso pues existe un producto de distintas funciones de $p$. Para facilitar este cálculo podemos utilizar una propiedad matemática que establece que si $f(x)$ tiene un punto extremo en $x_0$ y $f(x) > 0$ entonces $\ln f(x)$ también tiene un valor extremo en $x_0$. Por lo tanto podemos hacer uso de la *log-verosimilitud*:
	
$$\ell(p) = \ln \mathcal{L}(p) = u\ln p + (n-u) \ln (1-p)$$
	
$$\Rightarrow \frac{\partial \ln \mathcal{L}}{\partial p} =  \frac{u}{p} - \frac{n-u}{1-p} = 0 \Rightarrow \frac{u(1-p) - p(n-u)}{p(1-p)} = 0$$
	
$$\Rightarrow u - up - np + up = 0 \Rightarrow u = np \Rightarrow \hat{p} = \frac{u}{n} = \frac{\sum_{j=1}^{n} x_{j} }{n}$$
	
El valor crítico es $\hat{p} = \frac{\sum_{j=1}^{n} x_{j} }{n}$ pero falta demostrar que efectivamente sea un máximo.

## Estimadores de máxima verosimilitud
	
Su segunda derivada es:	
	
$$\frac{\partial^{2} \ln \mathcal{L}}{\partial p^{2}} = \frac{-u}{p^2} - \frac{n-u}{(1-p)^2} < 0,$$
cuando $p= \hat{p}$.


Por lo tanto, concluimos que $\hat{p}$ es el estimador de máxima verosimilitud. 



## Estimadores de máxima verosimilitud
	
		
**Ejemplo.** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria tal que $X_{j} \sim N(\mu, \sigma^2)$. Encuentre los estimadores máximo-verosímiles para $\mu$ y $\sigma^2$. 
	
**Solución.** Para no confunidr la notación, tome $\omega=\sigma^2$. Empezemos por obtener la función de verosimilitud:
	
$$\mathcal{L}(\mu, \omega) = \prod_{j=1}^{n} \frac{1}{\sqrt{2\pi\omega}} e^{-\frac{(x_{j}-\mu)^2}{2\omega}}= (2\pi)^{-\frac{n}{2}} (\omega)^{-\frac{n}{2}} e^{-\frac{\sum(x_{j} -\mu)^2}{2\omega}}$$
	
Obtenindo la log-verosimilitud:
	
$$\ell(\mu, \omega) = -\frac{n}{2} \ln(2\pi) -\frac{n}{2} \ln(\omega) -\frac{\sum(x_{j} -\mu)^2}{2\omega}$$
	
## Estimadores de máxima verosimilitud
	
		
Derivando con respecto a $\mu$:
	
$$\frac{\partial \ell(\mu,\omega)}{\partial \mu} = \frac{\sum(x_{j} -\mu)}{\omega} = 0 \Rightarrow \sum(x_{j} -\mu) = 0 \Rightarrow \hat{\mu} = \overline{X}$$
	
Su segunda deivada es $\frac{\partial^{2} \ell(\mu,\omega)}{\partial \mu^{2}} = -\frac{n}{\omega} < 0$

Ahora procedemos a derivar con respecto a $\omega$.
	
$$\frac{\partial \ell(\mu,\omega)}{\partial \omega} = -\frac{n}{2\omega} + \frac{\sum(x_{j} -\mu)^2}{2\omega^2} = \frac{-n\omega + \sum(x_{j} -\mu)^2}{2\omega^2} = 0$$
	
$$\Rightarrow \sum(x_{j} -\mu)^2 = n\omega \Rightarrow \hat{\omega} = \hat{\sigma}^{2} = \frac{\sum(x_{j} -\mu)^2}{n}$$
	
$$\frac{\partial^{2} \ell(\mu,\sigma^2)}{\partial \omega^{2}} = \frac{n}{2\omega^2} - \frac{\sum(x_{j} -\mu)^2}{\omega^3} < 0 \quad \text{, cuando} \quad \omega = \frac{\sum(x_{j} -\mu)^2}{n}$$



## Estimadores de máxima verosimilitud
	
		
Nótese que este estimador está en términos de un parámetro desconocido, $\mu$, por lo que podemos sustituirlo por su respectivo estimador de máxima verosimilitud. Por lo tanto obtenemos el estimador 
$$\hat{\sigma}^{2} = \frac{\sum(x_{j} -\overline{X})^2}{n} = \frac{(n-1)}{n} S^{2}$$
	
Concluimos que $\overline{X}$ y $\frac{(n-1)}{n} S^{2}$ son los estimadores de máxima verosimilitud de $\mu$ y $\sigma^2$, respectivamente. 

## Estimadores de máxima verosimilitud

Otra posibilidad es considerar la función de verosimilitud con dos parámetros desconocidos ($\mu$ y $\sigma^2$), es decir una función bivariada.

Tenemos que
$$\frac{\partial^{2} \ell(\mu,\omega)}{\partial \mu^{2}} = -\frac{n}{\sigma^2},~~~~~~~~~~~\frac{\partial^{2} \ell(\mu,\omega)}{\partial \omega^{2}} = \frac{n}{2\omega^2} - \frac{\sum(x_{j} -\mu)^2}{\omega^3}$$

$$\text{y}~~~\frac{\partial^{2} \ell(\mu,\omega)}{\partial \mu \partial \omega} =  - \frac{\sum(x_{j} -\mu)}{\omega^2}$$
La matriz hessiana es

$$ \begin{bmatrix}
   -\frac{n}{\sigma^2} & - \frac{\sum(x_{j} -\mu)}{\omega^2} \\
   - \frac{\sum(x_{j} -\mu)}{\omega^2} & \frac{n}{2\omega^2} - \frac{\sum(x_{j} -\mu)^2}{\omega^3} 
   \end{bmatrix}  $$

## Estimadores de máxima verosimilitud


El determinante de la matriz hessiana evaluada en $(\mu=\hat{\mu},~\omega=\hat{\omega})$ es:

$$ \left| \begin{bmatrix}
   -\frac{n}{\sigma^2} & - \frac{\sum(x_{j} -\mu)}{\omega^2} \\
   - \frac{\sum(x_{j} -\mu)}{\omega^2} & \frac{n}{2\omega^2} - \frac{\sum(x_{j} -\mu)^2}{\omega^3} 
   \end{bmatrix}  \right|_{(\mu=\hat{\mu},~\omega=\hat{\omega})}$$
$$=\left. \frac{1}{\omega^3}\left[ \frac{-n^2}{2} + \frac{n}{\omega}\sum_{i=1}^n (x_i-\mu)^2 - \frac{1}{\omega}\left( \sum_{i=1}^n (x_i-\mu) \right)^2 \right]\right|_{(\mu=\hat{\mu},~\omega=\hat{\omega})}$$
$$=\frac{1}{\hat{\omega}^3} \left[  \frac{-n^2}{2} + n^2 \right]= \frac{n^2}{2\hat{ \omega}^3}>0.$$
El determinante de la matriz hessiana es positivo, entonces el punto crítico encontrado es máximo o mínimo (no es punto de silla). Luego, para mostrar que es un máximo local, basta mostrar que las segundas derivadas parciales son negativas (ya se mostró anteriormente).


## Estimadores de máxima verosimilitud
	
		
**Ejemplo.** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria tal que $X_{j} \sim Unif(0, \theta)$. Encuentre el estimador de máxima verosimilitud de $\theta$. 
	
**Solución.** Empezemos por obtener la función de verosimilitud y log-verosimilitud:
	
$$\mathcal{L}(\theta) = \prod_{j=1}^{n} \frac{1}{\theta} = \theta^{-n}$$
	
$$\ell(\theta) = -n \ln\theta$$
	
Derivando con respecto a $\theta$:
	
$$\frac{\partial \ell (\theta)}{\partial \theta} = \frac{-n}{\theta} < 0$$



## Estimadores de máxima verosimilitud
	

Esto significa que $\ell (\theta)$ no tiene puntos críticos pero sí podemos encontrar un máximo local. Como es una función decreciente sabemos que el máximo se encuentra en el valor mínimo que $\theta$ puede tomar. Recordemos que la verosimilitud es una función de $\theta$ para un resultado específico de la muestra aleatoria, por lo que los valores de $\theta$ van a depender de los valores de la muestra aleatoria. Siendo $\theta$ el máximo poblacional es lógico pensar que para una muestra aleatoria dada lo mínimo que puede ser este valor es el máximo muestral $X_{(n)}$. Por lo tanto, concluimos que el estimador de máxima verosimilitud de $\theta$ es $X_{(n)}$. 



## Estimadores de máxima verosimilitud


![Figura 4. Verosimilitud de una $Unif(0,\theta)$ a partir de una muestra aleatoria.](figs/maxvero.jpg)	



## Propiedades de los estimadores de máxima verosimilitud
	
*	Si $\mathcal{L}(\theta)$ es función de verosimilitud entonces para un estadístico suficiente $U = T(X_{1}, X_{2}, ... , X_{n})$ para estimar $\theta$ se cumple, 
		
$$\mathcal{L}(\theta) = g(u,\theta)\cdot h(x_{1}, x_{2}, ... , x_{n})$$
$$\Rightarrow \ell(\theta) = \ln(g(u,\theta)) + \ln(h(x_{1}, x_{2}, ... , x_{n}))$$
$$\Rightarrow \frac{\partial \ell(\theta)}{\partial \theta} = \frac{g^{\prime}(u,\theta)}{g(u,\theta)}$$
		
Con lo anterior se demuestra que el EMV de $\theta$ debe ser función de $U$. Por el Teorema de Rao-Blackwell se cumple que son estimadores de variancia mínima. 
		
* Se puede demostrar que los EMV son asintóticamente insesgados. 
		
**Teorema 2.11:** *Principio de Invariancia:* Si $\hat{\theta}$ es EMV de $\theta$ y $t(\theta)$ es una función inyectiva de $\theta$ entonces $t(\hat{\theta})$ es el EMV de $t(\theta)$.



## Propiedades de los estimadores de máxima verosimilitud

**Ejemplo.** Sea $X_{1}, X_{2}, ... , X_{n}$ una muestra aleatoria Normal con $\mu$ y $\sigma^2$ desconocidos. Encuentre el EMV de $\sigma$. 
	
**Solución.** Se demostró con anterioridad que $\hat{\sigma}^{2} = \frac{(n-1)}{n} S^{2}$ es el EMV para $\sigma^2$. Sabiendo que la función $t(x) = \sqrt{x}$ es continua en los reales positivos entonces por la propiedad de Invariancia se cumple que $\sqrt{(\hat{\sigma^{2}})} = \sqrt{\frac{(n-1)}{n}}S$ es el EMV de $\sigma$. 



# Ejercicios

9.80 Suponga que $Y_1, Y_2, . . . , Y_n$ denotan una muestra aleatoria de la distribución de Poisson con media $\lambda$.

a. Encuentre el MLE $\hat{\lambda}$ para $\lambda$.

b. Encuentre el valor esperado y la varianza de $\hat{\lambda}$.

c. Demuestre que el estimador del inciso a es consistente para $\lambda$.

d. ¿Cuál es el MLE para $P(Y = 0) = \exp{(–\lambda)}$?

9.81 Suponga que $Y_1, Y_2, . . . , Y_n$ denotan una muestra aleatoria de una población distribuida exponencialmente con media $\theta$. Encuentre el MLE de la varianza poblacional $\theta^2$. [Sugerencia: recuerde el Ejemplo 9.9. de Mendenhall]

Suponga que $X_1, X_2, . . . , X_n$ es una m.a. de una población con densidad $f(x|\beta)=\displaystyle \frac {x}{\beta}e^{ -\frac{x^2}{2\beta}}$, con $x\geq 0, \ \beta > 0$. Determine el MLE para $\beta$.



# ¿Qué discutimos hoy?

Método de momentos. Método de máxima verosimilitud. Principio de invariancia. 

- Estimadores de momentos
- EMV
- Propiedades del EMV


