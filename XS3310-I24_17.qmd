---
title: "Alternativas no paramétricas-2"
subtitle: "XS3310 Teoría Estadística <br> I Semestre 2024"  
author: "Prof. Shu Wei Chou Chen"
institute: "Escuela de Estadística, UCR."
format:
  revealjs: 
    theme: simple
    slide-number: true
    fig-width: 3
    fig-height: 3
fontsize: 20pt
execute: 
  warning: false
---


## ¿Qué vamos a discutir hoy?

- Hemos visto hasta ahora sobre 
  - Estadística paramétrica
    - Estimadores puntuales
    - Intervalos de confianza.
    - Contrastes de hipótesis.
  - Estadística no paramétrica
    - Bootstrap
- Ahora: 
  - Estadística no paramétirca: Estimación de densidad kernel.

```{r}
library(ggplot2)
library(dplyr)
library(data.table)
```

```{css}
code {
  font-size: 1.5em;
  /* or try font-size: xx-large; */
}
```

## Distribución empírica

- Para una muestra $X_1, \dots, X_n$ de variables aleatorias con valores reales, independientes con distribución $P$, definimos la distribución $\hat{P}$ como:

$$\hat{P}(A) = \frac{1}{n}\sum_{i=1}^{n} I_A(X_i),$$ 
para $A \subseteq \mathbb{R}$ y $I_A(X_i)$ es la función indicadora definida como:

$$I_A(X_i)=\left\lbrace \begin{aligned}
				1 & \text{,  si  } X_i \in A,  \\
				0 & \text{,  si  } X_i \notin A.
\end{aligned} \right.$$

---

**Ejemplo:**
```{r}
#| echo: true
set.seed(1000)
(x=runif(10,0,5))
# A1= (0,1)
indicadoraA1 = between(x,0,1)
mean(indicadoraA1)
# A2= (1,2)
indicadoraA2 = between(x,1,2)
mean(indicadoraA2)
# A3= (2,3)
indicadoraA3 = between(x,2,3)
mean(indicadoraA3)
# A4= (3,4)
indicadoraA4 = between(x,3,4)
mean(indicadoraA4)

```

---

```{r}
#| echo: true

# A5= (4,5)
indicadoraA5 = between(x,4,5)
mean(indicadoraA5)

histograma = hist(x,breaks=c(0,1,2,3,4,5),freq=FALSE)
histograma$counts
histograma$density
```

## Histograma

- Para una muestra aleatoria $X_1, \dots, X_n$ de una población con función de densidad desconocida $f$.
- Escoja $x_0$ y $h$ el ancho del segmento y calcule los límites de cada segmento:
$$B_j = \left[ x_0 + (j-1)h, x_0+jh \right],~~~ j \in \mathbb{Z}.$$
- Cuente cuántas observaciones caen en cada segmento $j$, denotada por $n_j$:
- Para cada segmento $j$, calcule la frecuencia relativa

$$f_j = \frac{n_j}{nh}.$$
- Grafique el histograma usando barras de altura $f_j$ y ancho $h$.

---

**Ejemplo:**

:::: {.columns}

::: {.column width="44%" .add-space}
```{r}
#| echo: true
set.seed(1000)
n = 50
x=rnorm(n,10,2)
range(x)
h = 2.5
B1 = between(x,2.5,5)
n_1 = sum(B1)
B2 = between(x,5,7.5)
n_2 = sum(B2)
B3 = between(x,7.5,10)
n_3 = sum(B3)
B4 = between(x,10,12.5)
n_4 = sum(B4)
B5 = between(x,12.5,15)
n_5 = sum(B5)
B6 = between(x,15,17.5)
n_6 = sum(B6)
```
:::

::: {.column width="44%"}
```{r}
#| echo: true
n_j = c(n_1,n_2,n_3,n_4,n_5,n_6)
f_j = n_j/(n*h)
cbind(n_j,f_j)
```
:::
::::

---

```{r}
#| echo: true
histograma = hist(x,breaks=c(2.5,5,7.5,10,12.5,15,17.5),freq=FALSE)
histograma$counts
histograma$density
```



## Histograma

:::: {.columns}
::: {.column width="60%" .add-space}
- Formalmente, el histograma es dado por
$$\hat{f}_h(x)=\frac{1}{nh} \sum_{i=1}^n \sum_{j} I(X_i \in B_j) I(x \in B_j),$$
donde
$$I(X_i \in B_j)=\left\lbrace \begin{aligned}
				1 & \text{,  si  } X_i \in B_j,  \\
				0 & \text{,  si  } X_i \notin B_j.
\end{aligned} \right.$$
- Denote $m_j$ por el centro de cada segmento. Esto implica que la definición del histograma asigna para cada $x$ en el segmento $B_j=\left[m_j-\frac{h}{2},m_j+\frac{h}{2} \right)$ la misma estimación para $f$, $\hat{f}_h(m_j)$.
- Note que el área del histograma es $1$.

:::

::: {.column width="40%"}
**Ejemplo anterior:**

```{r}
#| fig-width: 4
#| fig-height: 4
hist(x,breaks=c(2.5,5,7.5,10,12.5,15,17.5),freq=FALSE)
```

```{r}
#| echo: true
sum(histograma$density*h)
```
:::
::::

---

**Ejemplo:** Variando $h$, el tamaño del segmento

```{r}
#| echo: true
set.seed(1000)
n = 100
x = rnorm(n,0,2)
(h1 = seq(-10,10,length.out=17))
(h2 = seq(-10,10,length.out=13))
(h3 = seq(-10,10,length.out=9))
(h4 = seq(-10,10,length.out=5))

```

---

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-width: 3
#| fig-height: 3
hist(x,breaks=h1,freq=FALSE)
hist(x,breaks=h2,freq=FALSE)
```
::: 

::: {.column width="50%"}

```{r}
#| echo: true
#| fig-width: 3
#| fig-height: 3
hist(x,breaks=h3,freq=FALSE)
hist(x,breaks=h4,freq=FALSE)
```
::: 
:::: 


## Propiedades estadística del histograma

- Si $x_0=0$, entonces los segmentos están dados por $B_j = \left[ (j-1)h, jh \right],~~~ j \in \mathbb{Z}$.
- Suponga que queremos estimar la densidad de un $x \in B_j$. La estimación usando el histograma para estimar $f(x)$ es
$$\hat{f}_h(x)=\frac{1}{nh} \sum_{i=1}^n \sum_{j} I(X_i \in B_j) I(x \in B_j)=\frac{1}{nh} \sum_{i=1}^n I(X_i \in B_j)$$
- $\hat{f}_h(x)$ es sesgado para estimar $f(x)$.

- Calculemos la esperanza del estimador $\hat{f}_h(x)$.
$$E\left(\hat{f}_h(x)\right)=\frac{1}{nh}\sum_{i=1}^n E\left\lbrace I(X_i \in B_j) \right\rbrace=\frac{1}{nh} n E\left\lbrace I(X_i \in B_j) \right\rbrace$$

---

- Note que $I(X_i \in B_j)$ es una variable aleatoria definida como

$$I(X_i \in B_j)= \begin{cases} 1, \text{ con probabilidad } \int_{(j-1)h}^{jh} f(u) du, \\
0,\text{ con probabilidad } 1-\int_{(j-1)h}^{jh} f(u) du\end{cases}$$

- Entonces, es un ensayo de Bernoulli y su esperanza es:
$$E\left\lbrace I(X_i \in B_j) \right\rbrace=\int_{(j-1)h}^{jh} f(u) du.$$
- Finalmente, tenemos que
$$E\left(\hat{f}_h(x)\right)=\frac{1}{nh} n E\left\lbrace I(X_i \in B_j) \right\rbrace=\frac{1}{h}  \int_{(j-1)h}^{jh} f(u) du.$$
- **El sesgo** es dado por

$$B\left(\hat{f}_h(x)\right)=E\left(\hat{f}_h(x)\right)-f(x)= \frac{1}{h}  \int_{(j-1)h}^{jh} f(u) du - f(x).$$

---

- Usando aproximación de Taylor de $f(x) - f(u)$ alrededor de $m_j=\left(j-\frac{1}{2}\right) h$ de $B_j$, tenemos $f(x) - f(u) \approx f'(m_j)\left[m_j-x\right]$. Por lo tanto,

$$B\left(\hat{f}_h(x)\right)=\frac{1}{h}  \int_{(j-1)h}^{jh} f(u) du - f(x)= \frac{1}{h}  \int_{(j-1)h}^{jh} f(u) - f(x)du \approx f'(m_j)\left[m_j-x\right].$$
**Observaciones:**

- El sesgo es casi cero cuando $x=m_j$, o sea en el punto medio de $B_j$.
- El sesgo depende de la pendiente de $f$.

---

- La variancia del estimador $\hat{f}_h(x)$:
$$Var\left(\hat{f}_h(x)\right)=Var \left\lbrace \frac{1}{nh}\sum_{i=1}^n  I(X_i \in B_j) \right\rbrace =\frac{1}{n^2h^2} Var \left\lbrace \sum_{i=1}^n  I(X_i \in B_j) \right\rbrace$$
$$=\frac{1}{n^2h^2} \left[\int_{(j-1)h}^{jh} f(u) du\right] \left[1-\int_{(j-1)h}^{jh} f(u) du\right] $$
Se puede mostrar que:
$$Var\left(\hat{f}_h(x)\right) \approx  \frac{1}{nh}f(x)$$
- Note que la variancia del estimador decrece cuando $nh$ crece, mientras que el sesgo del estimador decrece a cero si $h$ decrece.

---

- Recuerden que el **error cuadrático medio** es:
$$ECM\left(\hat{f}_h(x)\right)= Var\left(\hat{f}_h(x)\right)+\left[B\left(\hat{f}_h(x)\right)\right]^2.$$
- Para poder minimizar $ECM\left(\hat{f}_h(x)\right)$ se debe encontrar un equilibrio de $h$, pero esto sirve solamente para un $x$ dado.

- Existe el **error cuadrático medio integrado (ECMI)** definido como 
$$ECMI\left(\hat{f}_h(x)\right)=E\left\lbrace\int_{-\infty}^{\infty} \left[\hat{f}_h(x) - f(x) \right]^2 dx\right\rbrace$$
$$=\int_{-\infty}^{\infty} E\left[\left(\hat{f}_h(x) - f(x) \right)^2 \right] dx=\int_{-\infty}^{\infty} ECM\left(\hat{f}_h(x)\right) dx.$$
- Se puede comprobar que el óptimo ancho del segmento es:
$$h_{opt} \sim n^{-1/3}.$$

## Estimación de densidad Kernel

- Idea básica del histograma para estimar $f(x)$ es:
$$\hat{f}_h(x) = \frac{\#\left\lbrace \text{observaciones que caen dentro del intervalo que contiene a }x \right\rbrace}{n \cdot (\text{ancho del intervalo})},$$
donde el intervalo $B_i$ está centrado en $m_j$.

- La idea de la estimación de densidad por kernel es ligeramente diferente:

$$\hat{f}_k(x) = \frac{\#\left\lbrace \text{observaciones que caen dentro del intervalo que está alrededor de }x \right\rbrace}{n \cdot (\text{ancho del intervalo})}.$$

- Esto se puede reescribir como:
$$\hat{f}_k(x) = \frac{1}{n \cdot 2h} \#\left\lbrace X_i \in [x-h,x+h)] \right\rbrace$$

---

:::: {.columns}

::: {.column width="50%"}

- Si definimos **la función kernel uniforme**:

$$K(u)=\frac{1}{2} I(|u|\leq 1),$$
donde $u=(x-X_i)/h$.

- Podemos escribir ese estimador como:

$$\begin{align}
\hat{f}_k(x) &= \frac{1}{nh} \sum_{i=1}^n K\left( \frac{x-X_i}{h} \right) \\
&=\frac{1}{nh} \sum_{i=1}^n \frac{1}{2} I\left(\left|\frac{x-X_i}{h}\right|\leq 1\right).
\end{align}$$

::: 

::: {.column width="50%"}

- ¿Qué hace la función kernel uniforme?

```{r}
#| fig-width: 4
#| fig-height: 4
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "rectangular"), xlim = c(-4, 4),main="uniforme",xlab="")
```

:::
::::

---

**Ejemplo:** Para una muestra de una población con distribución desconocida: $(1,3,3.3,7,6.5,9)$.

```{r}
#| output: false
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="rectangular",bw=0.5)
density_df <- data.frame(value = kde$x, density = kde$y)

A.kernel<-sapply(x, function(i) {density(i,kernel="rectangular",bw=kde$bw)},simplify=F)
sapply(1:length(A.kernel), 
       function(i){
         A.kernel[[i]][['y']]<<-(A.kernel[[i]][['y']])/length(x)
        },
       simplify=F)

density_df_each <- list()
for(i in 1:length(A.kernel)){
  data <- A.kernel[[i]]
  density_df_each[[i]] <- data.frame(value = data$x, density = data$y, x = i)
}
density_df_each <- rbindlist(density_df_each)
density_df_each$x <- factor(density_df_each$x)

```


:::: {.columns}

::: {.column width="45%"}

```{r}
#| fig-width: 4
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw()
```

::: 

::: {.column width="55%"}

```{r}
#| fig-width: 6
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw() +
  geom_line(data=density_df_each, aes(x=value, y=density, color=x))
```

:::
::::


## Diferentes kernels

- Existe una variedad de funciones de kernels.

  - Uniforme: $K(u)=\frac{1}{2} I(|u|\leq 1),$
  - Triangular: $K(u)= (1-|u|) I(|u|\leq 1),$
  - Gaussiano: $K(u)=\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}u^2}$

Y otras como Epanechnikov, cudrática (biweight), triweight, coseno, etc.


:::: {.columns}

::: {.column width="30%"}
```{r}
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "rectangular"), xlim = c(-4, 4),main="uniforme",xlab="")
```
:::
::: {.column width="30%"}
```{r}
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "gaussian"), xlim = c(-4, 4),main="gaussian",xlab="")
```
::: 
::: {.column width="30%"}
```{r}
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "triangular"), xlim = c(-4, 4),main="triangular",xlab="")
```
::: 
::::

---

**Ejemplo:** Para una muestra de una población con distribución desconocida: $(1,3,3.3,7,6.5,9)$, estime la densidad usando el kernel gaussiano y $h=0.5$.

```{r}
#| output: false
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="gaussian",bw=0.5)
density_df <- data.frame(value = kde$x, density = kde$y)

A.kernel<-sapply(x, function(i) {density(i,kernel="gaussian",bw=kde$bw)},simplify=F)
sapply(1:length(A.kernel), 
       function(i){
         A.kernel[[i]][['y']]<<-(A.kernel[[i]][['y']])/length(x)
        },
       simplify=F)

density_df_each <- list()
for(i in 1:length(A.kernel)){
  data <- A.kernel[[i]]
  density_df_each[[i]] <- data.frame(value = data$x, density = data$y, x = i)
}
density_df_each <- rbindlist(density_df_each)
density_df_each$x <- factor(density_df_each$x)

```


:::: {.columns}

::: {.column width="45%"}

```{r}
#| fig-width: 4
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw()
```

::: 

::: {.column width="55%"}

```{r}
#| fig-width: 6
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw() +
  geom_line(data=density_df_each, aes(x=value, y=density, color=x))
```

:::
::::


---

**Ejemplo:** Para una muestra de una población con distribución desconocida: $(1,3,3.3,7,6.5,9)$, estime la densidad usando el kernel triangular y $h=0.5$.

```{r}
#| output: false
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="triangular",bw=0.5)
density_df <- data.frame(value = kde$x, density = kde$y)

A.kernel<-sapply(x, function(i) {density(i,kernel="triangular",bw=kde$bw)},simplify=F)
sapply(1:length(A.kernel), 
       function(i){
         A.kernel[[i]][['y']]<<-(A.kernel[[i]][['y']])/length(x)
        },
       simplify=F)

density_df_each <- list()
for(i in 1:length(A.kernel)){
  data <- A.kernel[[i]]
  density_df_each[[i]] <- data.frame(value = data$x, density = data$y, x = i)
}
density_df_each <- rbindlist(density_df_each)
density_df_each$x <- factor(density_df_each$x)

```


:::: {.columns}

::: {.column width="45%"}

```{r}
#| fig-width: 4
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw()
```

::: 

::: {.column width="55%"}

```{r}
#| fig-width: 6
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw() +
  geom_line(data=density_df_each, aes(x=value, y=density, color=x))
```
:::
::::

## Variando $h$


- Con $h=0.5,1,1.5,2$, lo que hace es suavizar más o menos la densidad estimada.

```{r}
#| fig-width: 6
#| fig-height: 5
kde1<-density(x,kernel="gaussian",bw=0.5)
kde2<-density(x,kernel="gaussian",bw=1)
kde3<-density(x,kernel="gaussian",bw=1.5)
kde4<-density(x,kernel="gaussian",bw=2)

density_df1 <- data.frame(value = kde1$x, density = kde1$y,h=0.5)
density_df2 <- data.frame(value = kde2$x, density = kde2$y,h=1)
density_df3 <- data.frame(value = kde3$x, density = kde3$y,h=1.5)
density_df4 <- data.frame(value = kde4$x, density = kde4$y,h=2)
density_df <- rbind(density_df1,density_df2,density_df3,density_df4)
density_df$h <- factor(density_df$h)
density_df %>% ggplot( aes(x = value, y = density,color=h)) + 
  geom_line() + theme_bw()

```

## En R

```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="gaussian",bw=0.5)
plot(kde)
```

---

```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
x<-c(1,3,3.3,7,6.5,9)
df<-data.frame(x)
ggplot(df, aes(x = x)) +
  geom_density(kernel="gaussian",bw=0.5) +
  theme_minimal()
```

## Propiedades estadística del estimador de densidad basado en kernels

- Se puede comprobar que el estimador $\hat{f}_k(x)$ es sesgado para estimar $f(x)$.

- No entraremos en detalles sobre los cálculos del sesgo, variancia y el ECM y ECMI.

- Se puede comprobar que el óptimo ancho del segmento es:
$$h_{opt} \sim n^{-1/5}.$$

## Ejemplo con la distribución Poisson

```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
x<-rpois(n=100, lambda= 10)
df<-data.frame(x)
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 2) +
  theme_minimal()
```

---



:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
ggplot(df, aes(x = x)) +
  geom_density(kernel="gaussian",bw=0.5) +
  theme_minimal()
```
:::
::: {.column width="50%"}
```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
ggplot(df, aes(x = x)) +
  geom_density(kernel="gaussian",bw=1.5) +
  theme_minimal()
```
::: 

::::




## ¿Qué discutimos hoy?

- Histograma
- Estimación de densidad por kernels.













